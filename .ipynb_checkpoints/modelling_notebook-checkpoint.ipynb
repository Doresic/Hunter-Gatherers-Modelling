{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question we want to ask:\n",
    "\n",
    "When is central-place foraging likely to emerge relative to point-to-point?\n",
    "\n",
    "\n",
    "\n",
    "Variables we are interested in testing for the emergence of CPF: (I am writing all)\n",
    "\n",
    "1. Sleeping sites distribution\n",
    "2. Resource type targeted (big/small game)\n",
    "3. Information-sharing\n",
    "4. Inter-forager correlation (how spatially correlated foragers are)\n",
    "5. Intra-forager variation (how successful a forager is)\n",
    "6. Resource sharing (between families in the same camp)\n",
    "7. Ability to rest (maybe it can be folded within resource sharing)\n",
    "8. Ability to defend against predators \n",
    "\n",
    "\n",
    "\n",
    "Our hypotheses:\n",
    "\n",
    "When is CPF better: when temporal, spatial, skill synchrony between foragers is beneficial, when there's a limited availability of sleeping sites, when there are multiple-co depending offspring, when resources are unpredictable and hard to find.\n",
    "\n",
    "When is P2P better: `**we need to spell out the potential costs of CPF and how to simulate them so that in some instances P2P is better**`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environmental variables\n",
    "\n",
    "\n",
    "class environment:\n",
    "\n",
    "\n",
    "##### global variables\n",
    "\n",
    "n_sleep_site = number of sleeping sites \n",
    "\n",
    "n_agents = number of foragers\n",
    "\n",
    "p_sharing = [0.1, 0.5, 0.9] - probability that an agent shares the harvested with others in their location at the end of the day\n",
    "\n",
    "n_large_res = number of large resources\n",
    "\n",
    "n_small_res = number of small resources\n",
    "\n",
    "pref_previous = y - some factor by which we multiply p_sharing to scale it depending on how many interactions\n",
    "\n",
    "p_info = [0.1, 0.5, 0.9] - probability that an agent shares information about location of resources with another agent\n",
    "\n",
    "mov_cost = cost of moving 1km \n",
    "\n",
    "min_carrying = minimum amount of energy an agent must have in order to carry a resource instead of consuming it\n",
    "\n",
    "##### resources \n",
    "\n",
    "type = small, large\n",
    "\n",
    "location = xy coordinates of resource\n",
    "\n",
    "prob_find = [0.1, 0.5] - probability to find each resource alone, unskilled\n",
    "\n",
    "prob_harvest = [0.1, 0.5] - probability to harvest each resource alone \n",
    "\n",
    "prop_energy = [0.1, 0.5] - proportion of daily requirement satisfied by each resource \n",
    "\n",
    "\n",
    "#### sleeping sites\n",
    "\n",
    "location = xy coordinates of sleeping site\n",
    "\n",
    "max_people = maximum number of foragers that can reside in a sleeping site\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent variables\n",
    "\n",
    "Agents are a family. E requirement of a family = 7,500kcal\n",
    "\n",
    "location = xy coordinates of agent\n",
    "\n",
    "carrying = number of resources agent is carrying\n",
    "\n",
    "for_speed = [3, 5] foraging speed - how many kilometers an agent can travel in 1h. \n",
    "\n",
    "max_carry = maximum amoung of resources that the agent can carry (will be the equivalent of 2 large resources or 4 small ones?)\n",
    "\n",
    "foraging distance (d) = [5, 10, 20] (km) how far the agent goes to find resources (maximum). small d will mean that they stay close to sleeping site and only find easily available resources like fruits. \n",
    "\n",
    "energy (e) : updated each day \n",
    "\n",
    "information (i): information for each resource = [0.1, 0.2, ...]. this will update whenever agent finds a resource and when it learns from someone else \n",
    "\n",
    "home (p_h): [x,x] tendency to return to sleeping site where they were the previous night (home i_d)\n",
    "\n",
    "#inter-forager correlation (c): [] length will be equal to num of agents (CP: i'd leave this for stage 2)\n",
    "\n",
    "p_f = tendency_to_forage: what proportion of days the agent goes foraging (can also be equivalent to what proportion of the members of the family goes foraging (we ignore inter-forager difference in skill?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each day:\n",
    "\n",
    "1. agent starts at a sleeping site\n",
    "2. it forages (8h/day foraging or moving)\n",
    "    - Agent starts foraging within d\n",
    "    - if a resource is found\n",
    "        - Resource is harvested with probability prob_harvested * i (information) of the agent over that resource\n",
    "    - pay cost of moving depending on how many km\n",
    "    - if resource is harvested\n",
    "        - agent carries the resource/consumes the resource depending on whether its energy is higher than min_carrying\n",
    "    - if agent is carrying less resources than max_carrying & has been foraging less than 8h\n",
    "        - agent continues foraging\n",
    "\n",
    "3. end of day: find sleeping site: \n",
    "    - pick a new site vs. one from previous day according to p_h\n",
    "    - if the site from previous day picked, \n",
    "        -   pay cost of moving back \n",
    "    - else\n",
    "        -   pay cost of moving to next available sleeping site\n",
    "\n",
    "4. at sleeping site:\n",
    "    - count how many foragers at that site \n",
    "    - share resources with others according to p_sharing and pref_previous\n",
    "    - share information with others with some probability (global variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporters\n",
    "\n",
    "Make plots of the following variables over time (over the course of the simulation)\n",
    "\n",
    "- Mean e of all agents (over time)\n",
    "- Variance in e of agents\n",
    "\n",
    "Make plots of the following variables at the end of the simulation: \n",
    "\n",
    "- Number of agents alive\n",
    "- Time til population collapses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.axes as axes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probabilities\n",
    "#P_SHARING = 0.3 # Can be 0.1, 0.5 or 0.9, probability of sharing harvested resources -- NOT USED\n",
    "#P_INFO = 0.3 # Can be 0.1, 0.5 or 0.9, probability of sharing information about resource locations -- NOT USED\n",
    "#PREF_PREVIOUS = 1.2 # Increase of sharing harvest/info probability with agents previously shared with \n",
    "PREF_PREVIOUS_MOVE = 0.7 # Probability of moving along the same direction as the previous move\n",
    "\n",
    "# Define costs\n",
    "MOVE_COST = 80 # Cost of moving 1 km in kcal (Google -- 1 km per kilogram of weight)\n",
    "CARRY_COST = 10 # Minimum energy in kcal needed to carry 1 unit of resource instead of consuming it on the spot (Around 10 kg -- 10 kcal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "SMALL_RES = 'small_res'\n",
    "LARGE_RES = 'large_res'\n",
    "\n",
    "ENERGY_REQUIREMENT = 7500 # Energy requirement for a day, in kcal\n",
    "\n",
    "SIM_TIME_STEP = 15 # Time step in minutes, determined by the time the fastest agent can move 1 km\n",
    "\n",
    "HIGH_PROB = 0.7\n",
    "MEDIUM_PROB = 0.5\n",
    "LOW_PROB = 0.3\n",
    "\n",
    "def distance(loc1: np.ndarray, loc2: np.ndarray):\n",
    "    \"\"\"Calculate the Manhattan distance between two locations.\"\"\"\n",
    "    return abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])\n",
    "\n",
    "def decision_making_measure(loc1: np.ndarray, loc2: np.ndarray, res_type):\n",
    "    \"\"\"Calculate the Manhattan distance between two locations, weighted by the type of resource.\n",
    "    \n",
    "    Distance is \"twice as small\" for large resources compared to small resources.\"\"\"\n",
    "    return (abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])) * (1 if res_type == SMALL_RES else 1/2)\n",
    "\n",
    "# Define necessary classes for the game/project/simulation\n",
    "\n",
    "class SleepSite():\n",
    "    def __init__(self, site_location: np.ndarray):\n",
    "        self.location: np.ndarray = site_location\n",
    "        self.residents: list[Agent] = []\n",
    "\n",
    "    def daily_reset(self) -> None:\n",
    "        \"\"\"Reset the sleep site at the start of the day.\"\"\"\n",
    "        # Remove all residents from the sleep site\n",
    "        self.residents = []\n",
    "    \n",
    "    def add_resident(self, agent) -> None:\n",
    "        \"\"\"Add an agent to the sleep site.\"\"\"\n",
    "        self.residents.append(agent)\n",
    "    \n",
    "    def share_harvested_resources(self) -> None:\n",
    "        \"\"\"Share the harvested resources between the residents of the sleep site: all residents get the same amount of resources.\"\"\"\n",
    "        # If there are no residents, do nothing\n",
    "        if len(self.residents) == 0:\n",
    "            return\n",
    "\n",
    "        # Collect all the food all residents have\n",
    "        all_resources: list[HarvestedResource] = []\n",
    "\n",
    "        for resident in self.residents:\n",
    "            all_resources.extend(resident.give_resources())\n",
    "        \n",
    "        # Compute the total prop_energy of all the resources\n",
    "        total_energy_prop = sum(res.consume() for res in all_resources)\n",
    "        total_energy = total_energy_prop * ENERGY_REQUIREMENT\n",
    "\n",
    "        # Divide the total energy between the residents\n",
    "        energy_per_resident = total_energy / len(self.residents)\n",
    "\n",
    "        # Add the energy to the residents\n",
    "        for resident in self.residents:\n",
    "            resident.gain_energy_from_resources(energy_per_resident)\n",
    "\n",
    "class HarvestedResource():\n",
    "    def __init__(self, res_type, prop_energy):\n",
    "        self.res_type = res_type\n",
    "        self.res_weight = 1 if self.res_type == SMALL_RES else 2 # Weight of the resource\n",
    "        self.energy = prop_energy\n",
    "\n",
    "    def consume(self) -> float:\n",
    "        \"\"\"Consume the resource. Returns the energy provided by the resource.\"\"\"\n",
    "        return self.energy\n",
    "\n",
    "class Resource():\n",
    "    def __init__(self, res_location: np.ndarray):\n",
    "        self.location: np.ndarray = res_location\n",
    "        \n",
    "        # Flip a coin for res_type, p_find, p_harvest, p_energy\n",
    "        self.res_type = SMALL_RES if random.random() < 0.5 else LARGE_RES\n",
    "        if self.res_type == SMALL_RES:\n",
    "            self.p_find = LOW_PROB if random.random() < 0.5 else HIGH_PROB # Probability of finding the resource, unskilled\n",
    "            self.p_harvest = MEDIUM_PROB if random.random() < 0.5 else HIGH_PROB # Probability of harvesting the resource, alone\n",
    "            self.prop_energy = 0.1 if random.random() < 0.5 else 0.5 # Proportion of daily energy consuming the resource provides\n",
    "            self.regrowth_time = 1 # Regrowth time in days.\n",
    "        elif self.res_type == LARGE_RES:\n",
    "            self.p_find = MEDIUM_PROB if random.random() < 0.5 else HIGH_PROB # Probability of finding a large resource is higher\n",
    "            self.p_harvest = LOW_PROB if random.random() < 0.5 else MEDIUM_PROB # Probability of harvesting a large resource is lower\n",
    "            self.prop_energy = 0.7 if random.random() < 0.5 else 0.9 # Large resource provides more energy\n",
    "            self.regrowth_time = 2 # Regrowth time in days, large resources take longer to regrow.\n",
    "\n",
    "        self.regrowing = False # Whether the resource is regrowing\n",
    "        self.regrowing_days_left = self.regrowth_time # Days left for the resource to regrow\n",
    "        self.res_weight = 1 if self.res_type == SMALL_RES else 2 # Weight of the resource\n",
    "\n",
    "    def search_for_resource(self) -> bool:\n",
    "        \"\"\"Search for the resource at the location. Returns True if the agent finds the resource, False otherwise.\"\"\"\n",
    "        # If the resource is regrowing, the agent cannot find it \n",
    "        #TODO: is this correct? Should the agent know the resource is regrowing?\n",
    "        if self.regrowing:\n",
    "            return False\n",
    "\n",
    "        # Roll a dice to see if the agent can find the resource\n",
    "        if random.random() < self.p_find: #TODO: include the agent's skill level?\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def harvest(self) -> HarvestedResource:\n",
    "        \"\"\"Harvest the resource at the location. Returns a HarvestedResource object if the agent manages to harvest the resource.\"\"\"\n",
    "        # Roll a dice to see if the agent can harvest the resource\n",
    "        if random.random() < self.p_harvest: # TODO: include the harvesting time -- connected to agent's skill level?\n",
    "            self.regrowing = True # The resource is regrowing as it was harvested\n",
    "            return HarvestedResource(self.res_type, self.prop_energy)\n",
    "\n",
    "    def grow(self) -> None:\n",
    "        \"\"\"Grow the resource at the location.\"\"\"\n",
    "        if self.regrowing_days_left > 0:\n",
    "            self.regrowing_days_left -= 1\n",
    "        else:\n",
    "            self.regrowing = False\n",
    "            self.regrowing_days_left = self.regrowth_time\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, agent_id: int, agent_location: np.ndarray, initial_sleep_site: SleepSite, grid_size: int):\n",
    "        self.id = agent_id # Unique identifier\n",
    "        self.location: np.ndarray = agent_location # Current location\n",
    "        self.grid_size = grid_size\n",
    "        self.energy = 0 # Energy level\n",
    "\n",
    "        self.harvested_resources: list[HarvestedResource] = []\n",
    "        self.capacity = 4 # Maximum number of resource weight the agent can carry # TODO: return if capacity is full?\n",
    "        self.knowledge: list[Resource] = [] # info of resource locations\n",
    "        self.known_sleep_sites: list[SleepSite] = [initial_sleep_site] # info of sleep sites\n",
    "        initial_sleep_site.add_resident(self) # Add the agent to the initial sleep site\n",
    "        # Assumed that the initial location is a sleep site\n",
    "\n",
    "        self.last_move_direction = None # Last move direction\n",
    "        self.last_sleep_site_loc = agent_location # Last sleep site\n",
    "\n",
    "        self.currently_moving = False # Whether the agent is currently moving\n",
    "        self.time_to_arrive = 0 # Time left to finish current move in minutes\n",
    "\n",
    "        self.desired_move_location = None # Desired move location\n",
    "        self.desired_loc_reached = False # Whether the desired location was reached\n",
    "\n",
    "        # Health reduces by percent of energy requirement not met\n",
    "        # and increases by percent of surplus energy percentage.\n",
    "        # If health is below 50, the agent has lower probability of finding/harvesting resources.\n",
    "        # If health reaches 0, the agent dies.\n",
    "        self.health = 100 # Health level\n",
    "        self.dead = False # Dead or alive\n",
    "\n",
    "        # Flip a coin for move_speed\n",
    "        self.move_speed = 2 if random.random() < 0.5 else 4 # Speed of the agent in km/h\n",
    "\n",
    "        # Flip a coin for maximum distance willing to be far from last sleep site\n",
    "        if random.random() < 1/2:\n",
    "            self.max_forage_dist = 10\n",
    "        else:\n",
    "            self.max_forage_dist = 20 #TODO are these ok values?\n",
    "        \n",
    "        # Flip a coin to see whether the agent always returns to the same sleep site\n",
    "        self.preferred_sleep_site = None if random.random() < 0.5 else initial_sleep_site\n",
    "\n",
    "        # TODO: Need some sort of memory?\n",
    "\n",
    "    def daily_reset(self) -> None:\n",
    "        \"\"\"Reset the agent's energy level and calculate health level at the start of next day.\"\"\"\n",
    "        if self.energy > ENERGY_REQUIREMENT:\n",
    "            self.health += (self.energy - ENERGY_REQUIREMENT) / ENERGY_REQUIREMENT\n",
    "            # Health cannot exceed 100\n",
    "            if self.health > 100:\n",
    "                self.health = 100\n",
    "\n",
    "        elif self.energy < ENERGY_REQUIREMENT:\n",
    "            self.health -= (ENERGY_REQUIREMENT - self.energy) / ENERGY_REQUIREMENT\n",
    "\n",
    "        if self.health < 0:\n",
    "            self.dead = True\n",
    "\n",
    "        self.energy = 0\n",
    "        self.last_move_direction = None\n",
    "        self.last_sleep_site_loc = self.location\n",
    "\n",
    "        self.currently_moving = False\n",
    "        self.time_to_arrive = 0\n",
    "    \n",
    "        self.desired_loc_reached = False\n",
    "        # With a probability 0.7, choose a new desired location, otherwise, do random foraging walks\n",
    "        if len(self.knowledge) > 0 and random.random() < 0.7:\n",
    "            # Sort the knowledge by weighted distance to the agent\n",
    "            self.knowledge.sort(key=lambda x: decision_making_measure(self.location, x.location, x.res_type))\n",
    "\n",
    "            # Choose the closest resource location which is inside self.max_forage_dist\n",
    "            for res in self.knowledge:\n",
    "                if distance(self.location, res.location) <= self.max_forage_dist:\n",
    "                    self.desired_move_location = res.location\n",
    "                    break\n",
    "        else:\n",
    "            self.desired_move_location = None\n",
    "\n",
    "\n",
    "    def daily_foraging_for_time_step(self, current_grid: list[list]) -> None:\n",
    "        \"\"\"Continue daily foraging for the next time step.\"\"\"\n",
    "\n",
    "        # Check if the agent is currently moving\n",
    "        if self.currently_moving:\n",
    "            self.time_to_arrive -= SIM_TIME_STEP\n",
    "\n",
    "            # If the agent has executed a move, stop and forage the location\n",
    "            if self.time_to_arrive <= 0:\n",
    "                self.currently_moving = False\n",
    "                self.time_to_arrive = 0\n",
    "\n",
    "                self.forage_location(current_grid)\n",
    "\n",
    "        # If the agent is not moving, choose the next move and start moving\n",
    "        else:\n",
    "            # If the agent has a desired location and it hasn't been reached yet, move towards it\n",
    "            if self.desired_move_location is not None and not self.desired_loc_reached:\n",
    "                # Move towards the desired location\n",
    "                self.move_towards_location(self.desired_move_location)\n",
    "                \n",
    "                # Check if the desired location was reached\n",
    "                if distance(self.location, self.desired_move_location) == 0:\n",
    "                    self.desired_loc_reached = True\n",
    "            else:\n",
    "                # Move randomly\n",
    "                self.move_randomly()\n",
    "            self.time_to_arrive = 60 / self.move_speed\n",
    "\n",
    "            # Use the time step to move\n",
    "            self.time_to_arrive -= SIM_TIME_STEP\n",
    "\n",
    "            if self.time_to_arrive <= 0:\n",
    "                self.currently_moving = False\n",
    "                self.time_to_arrive = 0\n",
    "            else:\n",
    "                self.currently_moving = True\n",
    "\n",
    "    def move_towards_location(self, desired_location) -> None:\n",
    "        \"\"\"Move towards the desired location.\"\"\"\n",
    "        # Calculate the direction to move\n",
    "        move_direction = desired_location - self.location\n",
    "        previous_location = copy.deepcopy(self.location)\n",
    "\n",
    "        # Do one step\n",
    "        if move_direction[0] == 0:\n",
    "            # Move along the y-axis if the x-axis is reached\n",
    "            self.location[1] += np.sign(move_direction[1])\n",
    "        elif move_direction[1] == 0:\n",
    "            # Move along the x-axis if the y-axis is reached\n",
    "            self.location[0] += np.sign(move_direction[0])\n",
    "        else:\n",
    "            # Prefer moving along the same direction as the last move\n",
    "            if self.last_move_direction is not None and random.random() < PREF_PREVIOUS_MOVE:\n",
    "                # Move along the same direction as the last move\n",
    "                self.location += self.last_move_direction\n",
    "            else:\n",
    "                # Move along the desired direction\n",
    "                if random.random() < 0.5:\n",
    "                    # Move along the x-axis\n",
    "                    self.location[0] += np.sign(move_direction[0])\n",
    "                else:\n",
    "                    # Move along the y-axis\n",
    "                    self.location[1] += np.sign(move_direction[1])\n",
    "\n",
    "        # Update the last move direction\n",
    "        self.last_move_direction = self.location - previous_location\n",
    " \n",
    "    def move_randomly(self) -> None:\n",
    "        \"\"\"Move randomly, but stay within the max_forage_distance.\"\"\"\n",
    "        # Randomly choose a direction to move, prefer moving along the same direction as the last move\n",
    "        if (\n",
    "            self.last_move_direction is not None \n",
    "            and random.random() < PREF_PREVIOUS_MOVE\n",
    "            and self.check_if_move_is_allowed(self.last_move_direction)\n",
    "        ):\n",
    "            # Move along the same direction as the last move\n",
    "            self.location += self.last_move_direction\n",
    "        else:\n",
    "            previous_location = copy.deepcopy(self.location)\n",
    "\n",
    "            possible_directions = [np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])]\n",
    "            allowed_directions = [pos_dir for pos_dir in possible_directions if self.check_if_move_is_allowed(pos_dir)]\n",
    "            self.location += random.choice(allowed_directions)\n",
    "\n",
    "            self.last_move_direction = self.location - previous_location\n",
    "        # TODO: should the agent be less likely to move in the opposite direction of the last move?  \n",
    "\n",
    "    def check_if_move_is_allowed(self, move_direction) -> bool:\n",
    "        \"\"\"Check whether the move moves the agent too far from the last sleep site or outside grid.\"\"\"\n",
    "        future_location = self.location + move_direction\n",
    "        if distance(self.last_sleep_site_loc, future_location) > self.max_forage_dist:\n",
    "            return False\n",
    "\n",
    "        if (\n",
    "            future_location[0] < 0 or future_location[0] >= self.grid_size\n",
    "            or future_location[1] < 0 or future_location[1] >= self.grid_size\n",
    "        ):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def gain_energy_from_resources(self, energy_gained) -> None:\n",
    "        \"\"\"Gain energy from the harvested resources.\"\"\"\n",
    "        self.energy += energy_gained\n",
    "     \n",
    "    def forage_location(self, grid: list[list]):\n",
    "        \"\"\"Forage the current location.\"\"\"\n",
    "        # Check if there's a sleep site at the location. If there is, save it in the known_sleep_sites\n",
    "        if grid[self.location[0]][self.location[1]] is not None:\n",
    "            if isinstance(grid[self.location[0]][self.location[1]], SleepSite):\n",
    "                self.known_sleep_sites.append(grid[self.location[0]][self.location[1]])\n",
    "\n",
    "        # Check if there's a resource at the location. \n",
    "        if grid[self.location[0]][self.location[1]] is not None:\n",
    "            if isinstance(grid[self.location[0]][self.location[1]], Resource):\n",
    "                resource : Resource = grid[self.location[0]][self.location[1]]\n",
    "                # Check if the agent can find the resource\n",
    "                if resource.search_for_resource():\n",
    "                    # Add the knowledge of the resource to the agent\n",
    "                    self.knowledge.append(resource)\n",
    "                    # Check if there is any capacity left to carry the resource\n",
    "                    if self.capacity >= sum([res.res_weight for res in self.harvested_resources]): #TODO: is this good to have?\n",
    "                        # Try to harvest the resource\n",
    "                        harvested_res = resource.harvest()\n",
    "                        if harvested_res is not None:\n",
    "                            self.harvested_resources.append(harvested_res)\n",
    "\n",
    "    def give_resources(self) -> list[HarvestedResource]:\n",
    "        \"\"\"Give out all harvested resources.\"\"\"\n",
    "        all_harvested_resources = self.harvested_resources\n",
    "\n",
    "        # Remove the resources from the agent\n",
    "        self.harvested_resources = []\n",
    "        return all_harvested_resources\n",
    "    \n",
    "    def go_to_sleep_site(self):\n",
    "        \"\"\"Go to a sleep site.\"\"\"\n",
    "        # If the agent has a preferred sleep site, go there\n",
    "        if self.preferred_sleep_site is not None:\n",
    "            self.location = copy.deepcopy(self.preferred_sleep_site.location)\n",
    "            # Add the agent to the sleep site as a resident\n",
    "            self.preferred_sleep_site.add_resident(self)\n",
    "        else:\n",
    "            # Choose the closest known sleep site\n",
    "            closest_known_sleep_site = min(\n",
    "                self.known_sleep_sites, key=lambda x: distance(self.location, x.location)\n",
    "            )\n",
    "            self.location = copy.deepcopy(closest_known_sleep_site.location)\n",
    "            # Add the agent to the sleep site as a resident\n",
    "            closest_known_sleep_site.add_resident(self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some General TODOs/Questions**\n",
    "- harvesting or foraging is not taking any time at the moment -- it's instant.\n",
    "- agent skill is not taken into account.\n",
    "- One can think that all both of the above are taken into account when choosing agent walking speed. That it encorporates all of those.\n",
    "- No sharing of knowledge about resource locations for now.\n",
    "- Energy spent on moving is not taken into account, including the moving to a sleeping site at the end of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the game simulator\n",
    "\n",
    "class Simulator():\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size: int,\n",
    "            n_agents: int,\n",
    "            n_resources: int,\n",
    "            n_sleep_sites: int,\n",
    "            verbose: bool = False, #TODO: currently only used for printing agent attributes\n",
    "    ):\n",
    "        self.grid = [[None for x in range(grid_size)] for y in range(grid_size)]\n",
    "        self.grid_size = grid_size\n",
    "\n",
    "        self.agents: list[Agent] = []\n",
    "        self.resources: list[Resource] = []\n",
    "        self.sleep_sites: list[SleepSite] = []\n",
    "\n",
    "        self.create_resources(n_resources)\n",
    "        self.create_sleep_sites(n_sleep_sites)\n",
    "        self.create_agents(n_agents, verbose=verbose)\n",
    "\n",
    "        # TODO: have history of the game, some sort of dataframe capturing the state of the game at each time step\n",
    "        \n",
    "\n",
    "    def create_agents(self, n_agents: int, verbose: bool = False) -> None:\n",
    "        \"\"\"Create n_agents number of agents.\"\"\"\n",
    "        if verbose:\n",
    "            # Initialize a dataframe with agent attributes\n",
    "            agent_attribute_df = pd.DataFrame(columns=['move_speed', 'max_forage_dist', 'preferred_sleep_site'])\n",
    "\n",
    "        for i in range(n_agents):\n",
    "            # For now, all agents start at the same location, the sleep site in the middle of the grid \n",
    "            agent_location = np.array([self.grid_size//2, self.grid_size//2])\n",
    "            self.agents.append(Agent(\n",
    "                agent_id=i,\n",
    "                agent_location=agent_location,\n",
    "                initial_sleep_site=self.sleep_sites[0],\n",
    "                grid_size=self.grid_size,\n",
    "            ))\n",
    "\n",
    "            if verbose: \n",
    "                # Add the agent attributes to the dataframe\n",
    "                agent_attribute_df.loc[i] = [\n",
    "                    self.agents[i].move_speed,\n",
    "                    self.agents[i].max_forage_dist,\n",
    "                    self.agents[i].preferred_sleep_site is not None,\n",
    "                ]\n",
    "        \n",
    "        if verbose:\n",
    "            display(agent_attribute_df)\n",
    "\n",
    "    def create_resources(self, n_resources: int) -> None:\n",
    "        \"\"\"Create resources in the grid.\"\"\"\n",
    "        for i in range(n_resources):\n",
    "\n",
    "            res_loc = np.array([random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1)])\n",
    "            while self.grid[res_loc[0]][res_loc[1]] is not None:\n",
    "                res_loc = np.array([random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1)])\n",
    "            \n",
    "            self.grid[res_loc[0]][res_loc[1]] = Resource(res_loc)\n",
    "            self.resources.append(self.grid[res_loc[0]][res_loc[1]])\n",
    "\n",
    "    def create_sleep_sites(self, n_sleep_sites) -> None:\n",
    "        \"\"\"Create sleep sites in the grid.\"\"\"\n",
    "        # Create the initial sleep site in the middle of the grid\n",
    "        self.grid[self.grid_size//2][self.grid_size//2] = SleepSite(np.array([self.grid_size//2, self.grid_size//2]))\n",
    "        self.sleep_sites.append(self.grid[self.grid_size//2][self.grid_size//2])\n",
    "\n",
    "        # Create the rest of the sleep sites\n",
    "        for i in range(n_sleep_sites - 1):\n",
    "            site_loc = np.array([random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1)])\n",
    "            while self.grid[site_loc[0]][site_loc[1]] is not None:\n",
    "                site_loc = np.array([random.randint(0, self.grid_size-1), random.randint(0, self.grid_size-1)])\n",
    "            self.grid[site_loc[0]][site_loc[1]] = SleepSite(site_loc)\n",
    "            self.sleep_sites.append(self.grid[site_loc[0]][site_loc[1]])\n",
    "\n",
    "    def daily_reset(self) -> None:\n",
    "        \"\"\"Reset the game at the start of the day.\"\"\"\n",
    "        # Reset all the sleep sites\n",
    "        for site in self.sleep_sites:\n",
    "            site.daily_reset()\n",
    "        \n",
    "        # Reset all the agents\n",
    "        for agent in self.agents:\n",
    "            agent.daily_reset()\n",
    "        \n",
    "        # Let the resources grow\n",
    "        for res in self.resources:\n",
    "            res.grow()\n",
    "    \n",
    "    def daily_foraging(\n",
    "            self,\n",
    "            verbose: bool = True,\n",
    "            all_state_artists: list = None,\n",
    "            ax = None,\n",
    "            day_number: int = 1,\n",
    "    ) -> Optional[animation.ArtistAnimation]:\n",
    "        \"\"\"Let the agents forage for the 8 hours of the day.\"\"\"\n",
    "        # Let the agents forage for 8 hours\n",
    "        # Iterate over time steps first\n",
    "        for _ in range(8 * 60 // SIM_TIME_STEP):\n",
    "            # Iterate over agents\n",
    "            for agent in self.agents: #TODO: dead agents do not forage. Remove them somehow to a \"cemetery\"?\n",
    "                agent.daily_foraging_for_time_step(self.grid)\n",
    "            if verbose:\n",
    "                # Save the current state of the game for the animation\n",
    "                all_state_artists.append(self.get_artists_for_current_state(ax, day_number))\n",
    "        \n",
    "        # All agents go to their sleep site at the end of the foraging time\n",
    "        for agent in self.agents:\n",
    "            agent.go_to_sleep_site()\n",
    "        all_state_artists.append(self.get_artists_for_current_state(ax, day_number))\n",
    "\n",
    "        if verbose:\n",
    "            # Animate the game\n",
    "            return all_state_artists\n",
    "\n",
    "    def simulate_day(\n",
    "            self,\n",
    "            verbose: bool = False,\n",
    "            all_state_artists: list = None,\n",
    "            fig = None,\n",
    "            ax = None,\n",
    "            day_number: int = 1,\n",
    "            multi_day_simulation: bool = False,\n",
    "    ) -> Optional[animation.ArtistAnimation]:\n",
    "        \"\"\"Simulate a day in the game.\"\"\"\n",
    "        if verbose and fig is None and ax is None and all_state_artists is None:\n",
    "            fig, ax = self.setup_figure()\n",
    "            # Store all game states artists for the animation\n",
    "            all_state_artists = []\n",
    "        elif not verbose:\n",
    "            all_state_artists = None\n",
    "        \n",
    "        if verbose:\n",
    "            all_state_artists.append(self.get_artists_for_current_state(ax, day_number))\n",
    "\n",
    "        # Reset the game at the start of the day\n",
    "        self.daily_reset()\n",
    "        if verbose:\n",
    "            print(\"Day reset\")\n",
    "\n",
    "        # Let the agents forage for the day\n",
    "        all_state_artists = self.daily_foraging(verbose, all_state_artists, ax, day_number)\n",
    "        if verbose:\n",
    "            print(\"Agents foraged\")\n",
    "\n",
    "        # Share the harvested resources between the residents of the sleep sites\n",
    "        for site in self.sleep_sites:\n",
    "            site.share_harvested_resources()\n",
    "        if verbose:\n",
    "            print(\"Resources shared at sleep sites\")\n",
    "        \n",
    "        if verbose and multi_day_simulation:\n",
    "            return all_state_artists\n",
    "        elif verbose:\n",
    "            print(\"Creating animation\")\n",
    "            return animation.ArtistAnimation(fig, all_state_artists, interval=200, blit=True, repeat_delay=1000)\n",
    "\n",
    "    def simulate_for_n_days(self, n_days: int, verbose: bool = False) -> Optional[animation.ArtistAnimation]:\n",
    "        \"\"\"Simulate the game for n_days.\"\"\"\n",
    "        if verbose:\n",
    "            print(\"Simulation started\")\n",
    "            fig, ax = self.setup_figure()\n",
    "            # Store all game states artists for the animation\n",
    "            all_state_artists = []\n",
    "\n",
    "        for day_number in range(1, n_days+1):\n",
    "            all_state_artists = self.simulate_day(\n",
    "                verbose=verbose,\n",
    "                all_state_artists=all_state_artists,\n",
    "                fig=fig,\n",
    "                ax=ax,\n",
    "                day_number=day_number,\n",
    "                multi_day_simulation=True,\n",
    "            )\n",
    "            if verbose:\n",
    "                print(f\"Day {day_number} simulated\", end='\\n\\n')      \n",
    "\n",
    "        if verbose:\n",
    "            print(\"Creating animation\")\n",
    "            return animation.ArtistAnimation(fig, all_state_artists, interval=200, blit=True, repeat_delay=1000)  \n",
    "\n",
    "    def get_artists_for_current_state(self, ax: axes, day_number: int) -> list:\n",
    "        \"\"\"Get the matplotlib artists for the current state of the game.\"\"\"\n",
    "        artists_for_current_state = []\n",
    "\n",
    "        # Plot the day number at the top of the figure\n",
    "        artists_for_current_state.append(\n",
    "            ax.text(self.grid_size//2, self.grid_size + 1, f'Day {day_number}', fontsize=12, ha='center', va='center')\n",
    "        )\n",
    "        \n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                if isinstance(self.grid[i][j], Resource):\n",
    "                    resource: Resource = self.grid[i][j]\n",
    "                    if resource.regrowing:\n",
    "                        alpha = 0.3\n",
    "                    else:\n",
    "                        alpha = 1\n",
    "\n",
    "                    if resource.res_type == SMALL_RES:\n",
    "                        artists_for_current_state.append(ax.scatter(i, j, color='green', marker='o', s=50, alpha=alpha))\n",
    "                    elif resource.res_type == LARGE_RES:\n",
    "                        artists_for_current_state.append(ax.scatter(i, j, color='green', marker='o', s=100, alpha=alpha))\n",
    "                \n",
    "                elif isinstance(self.grid[i][j], SleepSite):\n",
    "                    artists_for_current_state.append(ax.scatter(i, j, color='blue', marker='s'))\n",
    "                    # if there are residents, plot their number in the upper right corner of the sleep site\n",
    "                    if len(self.grid[i][j].residents) > 0:\n",
    "                        artists_for_current_state.append(\n",
    "                            ax.text(i+0.3, j+0.3, str(len(self.grid[i][j].residents)), fontsize=12, color='blue', ha='center', va='center')\n",
    "                        )\n",
    "                    else:\n",
    "                        artists_for_current_state.append(\n",
    "                            ax.text(i+0.3, j+0.3, '0', fontsize=12, color='blue', ha='center', va='center')\n",
    "                        )\n",
    "        \n",
    "        # Plot the agents\n",
    "        for agent in self.agents:\n",
    "            artists_for_current_state.append(ax.scatter(agent.location[0], agent.location[1], color='red', marker='x'))\n",
    "\n",
    "\n",
    "        return artists_for_current_state\n",
    "    \n",
    "    def setup_figure(self) -> tuple:\n",
    "        \"\"\"Setup the figure for the plotting or animation.\"\"\"\n",
    "        # Scale figure size with grid size\n",
    "        figure_size = max(6, 4 * (self.grid_size / 10))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(figure_size, figure_size))\n",
    "        \n",
    "        # Plot the boundaries of the grid\n",
    "        ax.set_xlim(-1, self.grid_size)\n",
    "        ax.set_ylim(-1, self.grid_size)\n",
    "        \n",
    "        # Plot the grid lines\n",
    "        ax.set_xticks(range(self.grid_size))\n",
    "        ax.set_yticks(range(self.grid_size))\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Plot legend\n",
    "        ax.scatter([], [], label='Small resource', s=50, color='green', marker='o')\n",
    "        ax.scatter([], [], label='Large resource', s=100, color='green', marker='o')\n",
    "        ax.scatter([], [], label='Regrowing small resource', s=50, color='green', alpha=0.3, marker='o')\n",
    "        ax.scatter([], [], label='Regrowing large resource', s=100, color='green', alpha=0.3, marker='o')\n",
    "        ax.scatter([], [], label='Sleep site', color='blue', marker='s')\n",
    "        ax.scatter([], [], label='Agent', color='red', marker='x')\n",
    "        ax.legend()\n",
    "\n",
    "        return fig, ax\n",
    "\n",
    "    def plot_current_state(self) -> None:\n",
    "        \"\"\"Plot the current state of the game.\"\"\"\n",
    "        # Plot the grid with resources and sleep sites\n",
    "        fig, ax = self.setup_figure()\n",
    "        figure_artists = self.get_artists_for_current_state(ax)\n",
    "\n",
    "        # Plot the figure using the artists\n",
    "        for artist in figure_artists:\n",
    "            ax.add_artist(artist)\n",
    "        plt.show()\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of locations: 400\n",
      "Number of agents: 3\n",
      "Number of resources: 40\n",
      "Number of sleep sites: 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>move_speed</th>\n",
       "      <th>max_forage_dist</th>\n",
       "      <th>preferred_sleep_site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   move_speed  max_forage_dist  preferred_sleep_site\n",
       "0           2               20                  True\n",
       "1           4               10                 False\n",
       "2           4               20                  True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation started\n",
      "Day reset\n",
      "Agents foraged\n",
      "Resources shared at sleep sites\n",
      "Day 1 simulated\n",
      "\n",
      "Day reset\n",
      "Agents foraged\n",
      "Resources shared at sleep sites\n",
      "Day 2 simulated\n",
      "\n",
      "Creating animation\n",
      "Showing the animation...\n"
     ]
    }
   ],
   "source": [
    "# Define the playing field: let it be a 20x20 grid -- 400 positions\n",
    "CHOSEN_GRID_SIZE = 10\n",
    "N_LOCATIONS = CHOSEN_GRID_SIZE * CHOSEN_GRID_SIZE\n",
    "print(\"Number of locations:\", N_LOCATIONS)\n",
    "\n",
    "# Define the global variables\n",
    "N_AGENTS = 6\n",
    "N_RESOURCES = int(N_LOCATIONS / 10)\n",
    "# N_LARGE_RES = N_LOCATIONS / 40 # Number of large resources -- NOT USED (DONE RANDOMLY)\n",
    "# N_SMALL_RES = N_LARGE_RES * 4 # Four times as many as large resources -- NOT USED (DONE RANDOMLY)\n",
    "N_SLEEP_SITES = int(N_LOCATIONS / 20)\n",
    "print(\"Number of agents:\", N_AGENTS)\n",
    "print(\"Number of resources:\", N_RESOURCES)\n",
    "print(\"Number of sleep sites:\", N_SLEEP_SITES)\n",
    "\n",
    "# Set numpy and random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize the simulator\n",
    "sim = Simulator(\n",
    "    grid_size=CHOSEN_GRID_SIZE,\n",
    "    n_agents=N_AGENTS,\n",
    "    n_resources=N_RESOURCES,\n",
    "    n_sleep_sites=N_SLEEP_SITES,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Plot the initial state\n",
    "#sim.plot_current_state()\n",
    "\n",
    "# Simulate the game for 1 day\n",
    "# FIXME: This is also failing actually. Currently moving thing is not working as intended...\n",
    "#ani = sim.simulate_day(verbose=True)\n",
    "\n",
    "# Simulate the game for 5 days\n",
    "# This is failing... #FIXME \n",
    "ani = sim.simulate_for_n_days(5, verbose=True)\n",
    "\n",
    "# Show the animation\n",
    "print(\"Showing the animation...\")\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save animation to gif\n",
    "#ani.save(\"day_animation.gif\", writer=\"imagemagick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
