{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question we want to ask:\n",
    "\n",
    "When is central-place foraging likely to emerge relative to point-to-point?\n",
    "\n",
    "\n",
    "\n",
    "Variables we are interested in testing for the emergence of CPF: (I am writing all)\n",
    "\n",
    "1. Sleeping sites distribution\n",
    "2. Resource type targeted (big/small game)\n",
    "3. Information-sharing\n",
    "4. Inter-forager correlation (how spatially correlated foragers are)\n",
    "5. Intra-forager variation (how successful a forager is)\n",
    "6. Resource sharing (between families in the same camp)\n",
    "7. Ability to rest (maybe it can be folded within resource sharing)\n",
    "8. Ability to defend against predators \n",
    "\n",
    "\n",
    "\n",
    "Our hypotheses:\n",
    "\n",
    "When is CPF better: when temporal, spatial, skill synchrony between foragers is beneficial, when there's a limited availability of sleeping sites, when there are multiple-co depending offspring, when resources are unpredictable and hard to find.\n",
    "\n",
    "When is P2P better: `**we need to spell out the potential costs of CPF and how to simulate them so that in some instances P2P is better**`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define environmental variables\n",
    "\n",
    "\n",
    "class environment:\n",
    "\n",
    "\n",
    "##### global variables\n",
    "\n",
    "n_sleep_site = number of sleeping sites \n",
    "\n",
    "n_agents = number of foragers\n",
    "\n",
    "p_sharing = [0.1, 0.5, 0.9] - probability that an agent shares the harvested with others in their location at the end of the day\n",
    "\n",
    "n_large_res = number of large resources\n",
    "\n",
    "n_small_res = number of small resources\n",
    "\n",
    "pref_previous = y - some factor by which we multiply p_sharing to scale it depending on how many interactions\n",
    "\n",
    "p_info = [0.1, 0.5, 0.9] - probability that an agent shares information about location of resources with another agent\n",
    "\n",
    "mov_cost = cost of moving 1km \n",
    "\n",
    "min_carrying = minimum amount of energy an agent must have in order to carry a resource instead of consuming it\n",
    "\n",
    "##### resources \n",
    "\n",
    "type = small, large\n",
    "\n",
    "location = xy coordinates of resource\n",
    "\n",
    "prob_find = [0.1, 0.5] - probability to find each resource alone, unskilled\n",
    "\n",
    "prob_harvest = [0.1, 0.5] - probability to harvest each resource alone \n",
    "\n",
    "prop_energy = [0.1, 0.5] - proportion of daily requirement satisfied by each resource \n",
    "\n",
    "\n",
    "#### sleeping sites\n",
    "\n",
    "location = xy coordinates of sleeping site\n",
    "\n",
    "max_people = maximum number of foragers that can reside in a sleeping site\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent variables\n",
    "\n",
    "Agents are a family. E requirement of a family = 7,500kcal\n",
    "\n",
    "location = xy coordinates of agent\n",
    "\n",
    "carrying = number of resources agent is carrying\n",
    "\n",
    "for_speed = [3, 5] foraging speed - how many kilometers an agent can travel in 1h. \n",
    "\n",
    "max_carry = maximum amoung of resources that the agent can carry (will be the equivalent of 2 large resources or 4 small ones?)\n",
    "\n",
    "foraging distance (d) = [5, 10, 20] (km) how far the agent goes to find resources (maximum). small d will mean that they stay close to sleeping site and only find easily available resources like fruits. \n",
    "\n",
    "energy (e) : updated each day \n",
    "\n",
    "information (i): information for each resource = [0.1, 0.2, ...]. this will update whenever agent finds a resource and when it learns from someone else \n",
    "\n",
    "home (p_h): [x,x] tendency to return to sleeping site where they were the previous night (home i_d)\n",
    "\n",
    "#inter-forager correlation (c): [] length will be equal to num of agents (CP: i'd leave this for stage 2)\n",
    "\n",
    "p_f = tendency_to_forage: what proportion of days the agent goes foraging (can also be equivalent to what proportion of the members of the family goes foraging (we ignore inter-forager difference in skill?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each day:\n",
    "\n",
    "1. agent starts at a sleeping site\n",
    "2. it forages (8h/day foraging or moving)\n",
    "    - Agent starts foraging within d\n",
    "    - if a resource is found\n",
    "        - Resource is harvested with probability prob_harvested * i (information) of the agent over that resource\n",
    "    - pay cost of moving depending on how many km\n",
    "    - if resource is harvested\n",
    "        - agent carries the resource/consumes the resource depending on whether its energy is higher than min_carrying\n",
    "    - if agent is carrying less resources than max_carrying & has been foraging less than 8h\n",
    "        - agent continues foraging\n",
    "\n",
    "3. end of day: find sleeping site: \n",
    "    - pick a new site vs. one from previous day according to p_h\n",
    "    - if the site from previous day picked, \n",
    "        -   pay cost of moving back \n",
    "    - else\n",
    "        -   pay cost of moving to next available sleeping site\n",
    "\n",
    "4. at sleeping site:\n",
    "    - count how many foragers at that site \n",
    "    - share resources with others according to p_sharing and pref_previous\n",
    "    - share information with others with some probability (global variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporters\n",
    "\n",
    "Make plots of the following variables over time (over the course of the simulation)\n",
    "\n",
    "- Mean e of all agents (over time)\n",
    "- Variance in e of agents\n",
    "\n",
    "Make plots of the following variables at the end of the simulation: \n",
    "\n",
    "- Number of agents alive\n",
    "- Time til population collapses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the playing field: let it be a 20x20 grid -- 400 positions\n",
    "GRID_SIZE = 20\n",
    "grid = [[None for x in range(GRID_SIZE)] for y in range(GRID_SIZE)]\n",
    "N_LOCATIONS = GRID_SIZE * GRID_SIZE\n",
    "\n",
    "# Define the global variables\n",
    "N_SLEEP_SITES = N_LOCATIONS / 10\n",
    "N_LARGE_RES = N_LOCATIONS / 40\n",
    "N_SMALL_RES = N_LARGE_RES * 4 # Four times as many as large resources\n",
    "\n",
    "\n",
    "# Define probabilities\n",
    "P_SHARING = 0.3 # Can be 0.1, 0.5 or 0.9, probability of sharing harvested resources\n",
    "P_INFO = 0.3 # Can be 0.1, 0.5 or 0.9, probability of sharing information\n",
    "PREF_PREVIOUS = 1.2 # Increase of sharing harvest/info probability with agents previously shared with\n",
    "PREF_PREVIOUS_MOVE = 0.7 # Probability of moving along the same direction as the previous move\n",
    "\n",
    "# Define costs\n",
    "MOVE_COST = 80 # Cost of moving 1 km in kcal (Google -- 1 km per kilogram of weight)\n",
    "CARRY_COST = 10 # Minimum energy in kcal needed to carry 1 unit of resource instead of consuming it on the spot (Around 10 kg -- 10 kcal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "SMALL_RES = 'small_res'\n",
    "LARGE_RES = 'large_res'\n",
    "\n",
    "ENERGY_REQUIREMENT = 7500 # Energy requirement for a day, in kcal\n",
    "\n",
    "HIGH_PROB = 0.7\n",
    "MEDIUM_PROB = 0.5\n",
    "LOW_PROB = 0.3\n",
    "\n",
    "def distance(loc1: np.ndarray, loc2: np.ndarray):\n",
    "    \"\"\"Calculate the Manhattan distance between two locations.\"\"\"\n",
    "    return abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])\n",
    "\n",
    "def resource_type_weighted_distance(loc1: np.ndarray, loc2: np.ndarray, res_type):\n",
    "    \"\"\"Calculate the Manhattan distance between two locations, weighted by the type of resource.\n",
    "    \n",
    "    Distance is \"twice as small\" for large resources compared to small resources.\"\"\"\n",
    "    return (abs(loc1[0] - loc2[0]) + abs(loc1[1] - loc2[1])) * (1 if res_type == SMALL_RES else 1/2)\n",
    "\n",
    "# Define necessary classes for the game/project/simulation\n",
    "\n",
    "class Resource():\n",
    "    def __init__(self, res_location: np.ndarray):\n",
    "        self.location: np.ndarray = res_location\n",
    "        \n",
    "        # Flip a coin for res_type, p_find, p_harvest, p_energy\n",
    "        self.res_type = SMALL_RES if random.random() < 0.5 else LARGE_RES\n",
    "        if self.res_type == SMALL_RES:\n",
    "            self.p_find = LOW_PROB if random.random() < 0.5 else HIGH_PROB # Probability of finding the resource, unskilled\n",
    "            self.p_harvest = MEDIUM_PROB if random.random() < 0.5 else HIGH_PROB # Probability of harvesting the resource, alone\n",
    "            self.prop_energy = 0.1 if random.random() < 0.5 else 0.5 # Proportion of daily energy consuming the resource provides\n",
    "        elif self.res_type == LARGE_RES:\n",
    "            self.p_find = MEDIUM_PROB if random.random() < 0.5 else HIGH_PROB # Probability of finding a large resource is higher\n",
    "            self.p_harvest = LOW_PROB if random.random() < 0.5 else MEDIUM_PROB # Probability of harvesting a large resource is lower\n",
    "            self.prop_energy = 0.7 if random.random() < 0.5 else 0.9 # Large resource provides more energy\n",
    "\n",
    "        self.res_weight = 1 if self.res_type == SMALL_RES else 2 # Weight of the resource\n",
    "\n",
    "    def search_for_resource(self):\n",
    "        \"\"\"Search for the resource at the location. Returns True if the agent finds the resource, False otherwise.\"\"\"\n",
    "        # Roll a dice to see if the agent can find the resource\n",
    "        if random.random() < self.p_find:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def harvest(self):\n",
    "        \"\"\"Harvest the resource at the location. Returns a HarvestedResource object if the agent manages to harvest the resource.\"\"\"\n",
    "        # Roll a dice to see if the agent can harvest the resource\n",
    "        if random.random() < self.p_harvest:\n",
    "            return HarvestedResource(self.res_type, self.prop_energy)\n",
    "\n",
    "class HarvestedResource():\n",
    "    def __init__(self, res_type, prop_energy):\n",
    "        self.res_type = res_type\n",
    "        self.res_weight = 1 if self.res_type == SMALL_RES else 2 # Weight of the resource\n",
    "        self.energy = prop_energy\n",
    "\n",
    "    def consume(self):\n",
    "        \"\"\"Consume the resource. Returns the energy provided by the resource.\"\"\"\n",
    "        return self.energy\n",
    "\n",
    "\n",
    "# TODO: If an agent doesn't find a resource, coming from knowledge from another agent, the probability of sharing with that agent reduces\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, agent_id: int, agent_location: np.ndarray):\n",
    "        self.id = agent_id # Unique identifier\n",
    "        self.location: np.ndarray = agent_location # Current location\n",
    "        self.energy = 0 # Energy level\n",
    "\n",
    "        self.harvested_resources: list[HarvestedResource] = []\n",
    "        self.capacity = 4 # Maximum number of resource weight the agent can carry\n",
    "        self.knowledge: list[Resource] = [] # info of resource locations\n",
    "\n",
    "        self.last_move_direction = None # Last move direction\n",
    "        self.last_sleep_site = agent_location # Last sleep site\n",
    "\n",
    "        self.desired_move_location = None # Desired move location\n",
    "        self.desired_loc_reached = False # Whether the desired location was reached\n",
    "\n",
    "        # Health reduces by percent of energy requirement not met\n",
    "        # and increases by percent of surplus energy percentage.\n",
    "        # If health is below 50, the agent has lower probability of finding/harvesting resources.\n",
    "        # If health reaches 0, the agent dies.\n",
    "        self.health = 100 # Health level\n",
    "        self.dead = False # Dead or alive\n",
    "\n",
    "        # Flip a coin for move_speed\n",
    "        self.move_speed = 3 if random.random() < 0.5 else 5 # Speed of the agent in km/h\n",
    "\n",
    "        # Flip a coin for maximum distance willing to be far from last sleep site\n",
    "        max_forage_dist_flip = random.random()\n",
    "        if max_forage_dist_flip < 1/3:\n",
    "            self.max_forage_dist = 5 \n",
    "        elif max_forage_dist_flip < 2/3:\n",
    "            self.max_forage_dist = 10\n",
    "        else:\n",
    "            self.max_forage_dist = 20 #TODO\n",
    "\n",
    "        # TODO: Need some sort of memory?\n",
    "    \n",
    "    def daily_reset(self):\n",
    "        \"\"\"Reset the agent's energy level and calculate health level at the start of next day.\"\"\"\n",
    "        if self.energy > ENERGY_REQUIREMENT:\n",
    "            self.health += (self.energy - ENERGY_REQUIREMENT) / ENERGY_REQUIREMENT\n",
    "            # Health cannot exceed 100\n",
    "            if self.health > 100:\n",
    "                self.health = 100\n",
    "\n",
    "        elif self.energy < ENERGY_REQUIREMENT:\n",
    "            self.health -= (ENERGY_REQUIREMENT - self.energy) / ENERGY_REQUIREMENT\n",
    "        \n",
    "        if self.health < 0:\n",
    "            self.dead = True\n",
    "        \n",
    "        self.energy = 0\n",
    "        self.last_move_direction = None\n",
    "    \n",
    "        self.desired_loc_reached = False\n",
    "        # With a probability 0.7, choose a new desired location, otherwise, do random foraging walks\n",
    "        if len(self.knowledge) > 0 and random.random() < 0.7:\n",
    "            # Sort the knowledge by weighted distance to the agent\n",
    "            self.knowledge.sort(key=lambda x: resource_type_weighted_distance(self.location, x.location, x.res_type))\n",
    "\n",
    "            # Choose the closest resource location which is inside self.max_forage_dist\n",
    "            for res in self.knowledge:\n",
    "                if distance(self.location, res.location) <= self.max_forage_dist:\n",
    "                    self.desired_move_location = res.location\n",
    "                    break\n",
    "        else:\n",
    "            self.desired_move_location = None\n",
    "        \n",
    "        \n",
    "    def daily_foraging(self, new_location):\n",
    "        \"\"\"Execute the daily 8-hour foraging of the agent.\"\"\"\n",
    "        steps_left_to_take = self.move_speed * 8\n",
    "\n",
    "        while steps_left_to_take > 0:\n",
    "            steps_left_to_take -= 1\n",
    "\n",
    "            # If the agent has a desired location and it hasn't been reached yet, move towards it\n",
    "            if self.desired_move_location is not None and not self.desired_loc_reached:\n",
    "                # Move towards the desired location\n",
    "                self.move_towards_location(self.desired_move_location)\n",
    "                \n",
    "                # Check if the desired location was reached\n",
    "                if self.location == self.desired_move_location:\n",
    "                    self.desired_loc_reached = True\n",
    "\n",
    "            else:\n",
    "                # Move randomly\n",
    "                self.move_randomly()\n",
    "                steps_left_to_take -= 1\n",
    "            \n",
    "            # Forage the current location\n",
    "            self.forage_location()\n",
    "\n",
    "            # TODO:\n",
    "\n",
    "\n",
    "\n",
    "    def move_towards_location(self, desired_location):\n",
    "        \"\"\"Move towards the desired location.\"\"\"\n",
    "        # Calculate the direction to move\n",
    "        move_direction = desired_location - self.location\n",
    "        previous_location = copy.deepcopy(self.location)\n",
    "\n",
    "        # Do one step\n",
    "        if move_direction[0] == 0:\n",
    "            # Move along the y-axis if the x-axis is reached\n",
    "            self.location[1] += np.sign(move_direction[1])\n",
    "        elif move_direction[1] == 0:\n",
    "            # Move along the x-axis if the y-axis is reached\n",
    "            self.location[0] += np.sign(move_direction[0])\n",
    "        else:\n",
    "            # Prefer moving along the same direction as the last move\n",
    "            if self.last_move_direction is not None and random.random() < PREF_PREVIOUS_MOVE:\n",
    "                # Move along the same direction as the last move\n",
    "                self.location += self.last_move_direction\n",
    "            else:\n",
    "                # Move along the desired direction\n",
    "                if random.random() < 0.5:\n",
    "                    # Move along the x-axis\n",
    "                    self.location[0] += np.sign(move_direction[0])\n",
    "                else:\n",
    "                    # Move along the y-axis\n",
    "                    self.location[1] += np.sign(move_direction[1])\n",
    "\n",
    "        # Update the last move direction\n",
    "        self.last_move_direction = self.location - previous_location\n",
    "    \n",
    "    def move_randomly(self):\n",
    "        \"\"\"Move randomly.\"\"\"\n",
    "        # Randomly choose a direction to move, prefer moving along the same direction as the last move\n",
    "        if self.last_move_direction is not None and random.random() < PREF_PREVIOUS_MOVE:\n",
    "            # Move along the same direction as the last move\n",
    "            self.location += self.last_move_direction\n",
    "        else:\n",
    "            # Move randomly\n",
    "            move_direction = np.random.choice([np.array([1, 0]), np.array([-1, 0]), np.array([0, 1]), np.array([0, -1])])\n",
    "            self.location += move_direction\n",
    "            self.last_move_direction = move_direction\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def consume_resouce(self):\n",
    "        # Consume the resource\n",
    "        self.energy += self.prop_energy * ENERGY_REQUIREMENT\n",
    "        # TODO: not finished\n",
    "        \n",
    "    def forage_location(self):\n",
    "        # TODO: not finished\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
